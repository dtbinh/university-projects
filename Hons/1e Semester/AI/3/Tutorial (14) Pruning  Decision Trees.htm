<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN" "http://www.w3c.org/TR/1999/REC-html401-19991224/loose.dtd">
<!-- saved from url=(0032)http://decisiontrees.net/node/44 -->
<HTML lang=en xml:lang="en" xmlns="http://www.w3.org/1999/xhtml"><HEAD><TITLE>Tutorial (14): Pruning | Decision Trees</TITLE>
<META http-equiv=Content-Style-Type content=text/css>
<META http-equiv=Content-Type content="text/html; charset=utf-8">
<STYLE type=text/css media=all>@import url( misc/drupal.css );
</STYLE>

<STYLE type=text/css media=all>@import url( themes/friendselectric/style.css );
</STYLE>

<SCRIPT type=text/javascript> </SCRIPT>

<META content="MSHTML 6.00.2900.2873" name=GENERATOR></HEAD>
<BODY>
<DIV class=bw1>
<DIV class=bw2>
<DIV id=body-wrap>
<DIV id=header>
<DIV class=hw1>
<DIV class=hw2><A title="Index Page" href="http://decisiontrees.net/"><IMG 
id=site-logo alt=Logo 
src="Tutorial (14) Pruning  Decision Trees_files/logo.png"></A> 
<H1 class=without-slogan id=site-name><A title="Index Page" 
href="http://decisiontrees.net/">Decision Trees</A></H1>
<DIV id=top-nav>
<UL id=primary>
  <LI><A title="Interactive tutorial" 
  href="http://decisiontrees.net/node/16"><SPAN class=lw1><SPAN 
  class=lw2>Tutorial</SPAN></SPAN></A> </LI>
  <LI><A title=Books href="http://decisiontrees.net/node/19"><SPAN 
  class=lw1><SPAN class=lw2>Books</SPAN></SPAN></A> </LI>
  <LI><A title=Software href="http://decisiontrees.net/node/22"><SPAN 
  class=lw1><SPAN class=lw2>Software</SPAN></SPAN></A> </LI>
  <LI><A title=Sites href="http://decisiontrees.net/node/23"><SPAN 
  class=lw1><SPAN class=lw2>Sites</SPAN></SPAN></A> </LI>
  <LI><A title=Papers href="http://decisiontrees.net/node/45"><SPAN 
  class=lw1><SPAN class=lw2>Papers</SPAN></SPAN></A> </LI>
  <LI><A title=Forums href="http://decisiontrees.net/forum"><SPAN 
  class=lw1><SPAN class=lw2>Forums</SPAN></SPAN></A> </LI>
  <LI><A title=About href="http://decisiontrees.net/node/42"><SPAN 
  class=lw1><SPAN class=lw2>About</SPAN></SPAN></A> 
</LI></UL></DIV></DIV></DIV></DIV>
<DIV class=content-both id=content>
<DIV class=cw1>
<DIV class=cw2>
<DIV class=cw3>
<DIV class=cw4>
<DIV class=cw5>
<DIV class=cw6>
<DIV class=cw7>
<DIV class=cw8>
<DIV class=content-wrap-both id=content-wrap>
<DIV class=sidebar id=sidebar-left>
<DIV class="block block-user" id=block-user-0>
<H2 class=first>User login</H2>
<DIV class=content>
<FORM action=user/login?destination=node%2F44 method=post>
<DIV class=user-login-block>
<DIV class=form-item><LABEL for=edit-name>Username:</LABEL><BR><INPUT 
class=form-text id=edit-name maxLength=64 size=15 name=edit[name]> </DIV>
<DIV class=form-item><LABEL for=edit-pass>Password:</LABEL><BR><INPUT 
class=form-password id=edit-pass type=password maxLength=64 size=15 
name=edit[pass]> </DIV><INPUT class=form-submit type=submit value="Log in" name=op> </DIV></FORM>
<DIV class=item-list>
<UL>
  <LI><A title="Create a new user account." 
  href="http://decisiontrees.net/user/register">Create new account</A>
  <LI><A title="Request new password via e-mail." 
  href="http://decisiontrees.net/user/password">Request new 
password</A></LI></UL></DIV></DIV></DIV>
<DIV class="block block-book" id=block-book-0>
<H2>Tutorial</H2>
<DIV class=content>
<DIV class=menu>
<UL>
  <LI class=leaf><A href="http://decisiontrees.net/node/21">Tutorial (1): A 
  simple decision tree</A>
  <LI class=leaf><A href="http://decisiontrees.net/node/25">Tutorial (2): 
  Exercise 1</A>
  <LI class=leaf><A href="http://decisiontrees.net/node/26">Tutorial (3): 
  Occam's Razor</A>
  <LI class=leaf><A href="http://decisiontrees.net/node/27">Tutorial (4): 
ID3</A>
  <LI class=leaf><A href="http://decisiontrees.net/node/28">Tutorial (5): 
  Exercise 2</A>
  <LI class=leaf><A href="http://decisiontrees.net/node/29">Tutorial (6): 
  Entropy Bias</A>
  <LI class=leaf><A href="http://decisiontrees.net/node/31">Tutorial (7): 
  Exercise 3</A>
  <LI class=leaf><A href="http://decisiontrees.net/node/32">Tutorial (8): Other 
  Splitting Criteria</A>
  <LI class=leaf><A href="http://decisiontrees.net/node/33">Tutorial (9): 
  Exercise 4</A>
  <LI class=leaf><A href="http://decisiontrees.net/node/34">Tutorial (10): 
  Advanced Topics</A>
  <LI class=leaf><A href="http://decisiontrees.net/node/35">Tutorial (11): 
  Evaluating Decision Trees</A>
  <LI class=leaf><A href="http://decisiontrees.net/node/36">Tutorial (12): 
  Exercise 5</A>
  <LI class=leaf><A href="http://decisiontrees.net/node/37">Tutorial (13): 
  Overfitting</A>
  <LI class=leaf><A class=active 
  href="http://decisiontrees.net/node/44">Tutorial (14): Pruning</A>
  <LI class=leaf><A href="http://decisiontrees.net/node/38">Tutorial (15): 
  Exercise 6</A>
  <LI class=leaf><A href="http://decisiontrees.net/node/39">Tutorial (16): 
  Further Topics</A>
  <LI class=leaf><A href="http://decisiontrees.net/node/40">Tutorial (17): 
  Conclusion</A></LI></UL></DIV></DIV></DIV>
<DIV class="block block-user" id=block-user-1>
<H2>Navigation</H2>
<DIV class=content>
<DIV class=menu>
<UL>
  <LI class=collapsed><A href="http://decisiontrees.net/node/add"><SPAN 
  class=lw1>create content</SPAN></A> </LI></UL></DIV></DIV></DIV>
<DIV class="block block-forum" id=block-forum-0>
<H2>Active forum topics</H2>
<DIV class=content>
<DIV class=item-list>
<UL>
  <LI><A href="http://decisiontrees.net/node/47">Machine Learning Data 
  Sets</A></LI></UL></DIV>
<DIV class=more-link><A title="Read the latest forum topics." 
href="http://decisiontrees.net/forum">more</A></DIV></DIV></DIV>
<DIV class="block block-comment" id=block-comment-0>
<H2>Recent comments</H2>
<DIV class=content>
<DIV class=item-list>
<UL>
  <LI><A href="http://decisiontrees.net/node/20#comment-17">automatic 
  interaction detection</A><BR>8 weeks 3 days ago
  <LI><A href="http://decisiontrees.net/node/20#comment-16">Data sets</A><BR>8 
  weeks 6 days ago
  <LI><A href="http://decisiontrees.net/node/20#comment-15">CHAID AND 
  AID</A><BR>9 weeks 1 hour ago
  <LI><A href="http://decisiontrees.net/node/20#comment-14">re: CHAID and 
  AID</A><BR>9 weeks 2 hours ago
  <LI><A href="http://decisiontrees.net/node/20#comment-13">CHAID and 
  AID</A><BR>9 weeks 5 hours ago
  <LI><A href="http://decisiontrees.net/node/45#comment-12">Large database 
  decision tree classifiers</A><BR>9 weeks 3 days ago
  <LI><A href="http://decisiontrees.net/node/22#comment-9">Re:See5</A><BR>10 
  weeks 15 hours ago
  <LI><A href="http://decisiontrees.net/node/45#comment-8">Need papers about 
  data mining</A><BR>10 weeks 22 hours ago
  <LI><A href="http://decisiontrees.net/node/22#comment-7">See5</A><BR>10 weeks 
  22 hours ago
  <LI><A href="http://decisiontrees.net/node/20#comment-6">Access 
  problems?</A><BR>10 weeks 22 hours ago</LI></UL></DIV></DIV></DIV>
<DIV class="block block-user" id=block-user-3>
<H2>Who's online</H2>
<DIV class=content>There are currently 0 users and 12 guests 
online.</DIV></DIV></DIV>
<DIV class=main-both id=main>
<DIV class=main-wrap-both id=main-wrap>
<DIV class=mw1>
<DIV class=breadcrumb><A href="http://decisiontrees.net/">Home</A> » <A 
href="http://decisiontrees.net/node/16"><SPAN 
class=lw1>Tutorial</SPAN></A></DIV>
<H2 class=main-title>Tutorial (14): Pruning</H2><!-- begin content -->
<DIV class=node>
<DIV class=content><BR>
<STYLE>TABLE {
	BORDER-COLLAPSE: separate
}
TH {
	PADDING-RIGHT: 0px; PADDING-LEFT: 0px; PADDING-BOTTOM: 0px; MARGIN: 0px; BORDER-TOP-STYLE: none; PADDING-TOP: 0px; BORDER-RIGHT-STYLE: none; BORDER-LEFT-STYLE: none; BORDER-BOTTOM-STYLE: none
}
TR {
	PADDING-RIGHT: 0px; PADDING-LEFT: 0px; PADDING-BOTTOM: 0px; MARGIN: 0px; PADDING-TOP: 0px; BORDER-BOTTOM-STYLE: none
}
TBODY {
	TEXT-ALIGN: center
}
TABLE TD {
	PADDING-RIGHT: 0px; PADDING-LEFT: 0px; PADDING-BOTTOM: 0px; MARGIN: 0px; PADDING-TOP: 0px
}
TABLE {
	MARGIN: 0.5em 0px 1em
}
TBODY {
	MARGIN-LEFT: 3px; TEXT-ALIGN: center
}
#tree {
	
}
#popupMenu {
	BORDER-RIGHT: #ccc 1px solid; PADDING-RIGHT: 2px; BORDER-TOP: #ccc 1px solid; PADDING-LEFT: 2px; FONT-WEIGHT: bold; VISIBILITY: hidden; PADDING-BOTTOM: 2px; BORDER-LEFT: #ccc 1px solid; PADDING-TOP: 2px; BORDER-BOTTOM: #ccc 1px solid; POSITION: absolute; BACKGROUND-COLOR: #fafafa
}
.headers {
	PADDING-RIGHT: 2px; PADDING-LEFT: 2px; FONT-WEIGHT: bold; PADDING-BOTTOM: 2px; WIDTH: 50%; PADDING-TOP: 2px; BACKGROUND-COLOR: #dbe8f0
}
.subheaders {
	PADDING-RIGHT: 2px; PADDING-LEFT: 2px; FONT-WEIGHT: bold; PADDING-BOTTOM: 2px; MARGIN-LEFT: 3px; WIDTH: 75%; PADDING-TOP: 2px
}
.build {
	BACKGROUND-COLOR: #6699ff
}
.prune {
	BACKGROUND-COLOR: #ff3300
}
.correctColor {
	BACKGROUND-COLOR: #a0c0d6
}
.wrongColor {
	BACKGROUND-COLOR: #fbaac5
}
.exNode {
	PADDING-RIGHT: 2px; PADDING-LEFT: 2px; FONT-WEIGHT: bold; PADDING-BOTTOM: 2px; PADDING-TOP: 2px; BACKGROUND-COLOR: #e7cc9a
}
.exNode:hover {
	BACKGROUND-COLOR: #b49967
}
.normalNode {
	BORDER-RIGHT: #ffffff 1px solid; PADDING-RIGHT: 2px; BORDER-TOP: #ffffff 1px solid; PADDING-LEFT: 2px; FONT-WEIGHT: normal; PADDING-BOTTOM: 2px; BORDER-LEFT: #ffffff 1px solid; PADDING-TOP: 2px; BORDER-BOTTOM: #ffffff 1px solid; BACKGROUND-COLOR: #a0c0d6
}
.normalNode:hover {
	PADDING-RIGHT: 2px; PADDING-LEFT: 2px; PADDING-BOTTOM: 2px; PADDING-TOP: 2px; BACKGROUND-COLOR: #dbe8f0
}
.leafNode {
	PADDING-RIGHT: 2px; PADDING-LEFT: 2px; FONT-WEIGHT: normal; PADDING-BOTTOM: 2px; PADDING-TOP: 2px; BACKGROUND-COLOR: #92f986
}
.leafNode:hover {
	BACKGROUND-COLOR: #68c652
}
.branchNode {
	
}
.normalNode1 {
	PADDING-RIGHT: 2px; PADDING-LEFT: 2px; FONT-WEIGHT: normal; PADDING-BOTTOM: 2px; PADDING-TOP: 2px; BACKGROUND-COLOR: #cc99cc
}
.correctStat {
	COLOR: #00cc66
}
.wrongStat {
	COLOR: #f8341f
}
#dataTable {
	CURSOR: pointer
}
#validTable {
	CURSOR: pointer
}
</STYLE>

<H2>Pruning to avoid overfitting </H2>
<P>The approach to constructing decision trees usually involves using <SPAN 
class=titles>greedy heuristics</SPAN> (such as Entropy reduction) that overfit 
the training data and lead to poor accuracy in future predictions. </P>
<P>In response to the problem of overfitting nearly all modern decision tree 
algorithms adopt a pruning strategy of some sort. Many algorithms use a 
technique known as postpruning or backward pruning. This essentially involves 
growing the tree from a dataset until all possible leaf nodes have been reached 
(i.e. purity) and then removing particular substrees. Studies have shown that 
post-pruning will result in smaller and more accurate trees by up to 25%. 
Different pruning techniques have been developed which have been compared in 
several papers and like with the different splitting criteria it has been found 
that there is not much variation in terms of performance (e.g. see Mingers89 and 
Esposito et. al. 97). There are quite a few methods that have been developed. 
We'll look at one of the basic ones here. </P>
<H3>Pruning strategies</H3>
<P class=titles>An example: Reduced Error Pruning (Quinlan 86) </P>
<P>At each node in a tree it is possible to see the number of instances that are 
misclassified on a testing set by propagating errors upwards from leaf nodes. 
This can be compared to the error-rate if the node was replaced by the most 
common class resulting from that node. If the difference is a reduction in 
error, then the subtree at the node can be considered for pruning. This 
calculation is performed for all nodes in the tree and whichever one has the 
highest reduced-error rate is pruned. The procedure is then recursed over the 
freshly pruned tree until there is no possible reduction in error rate at any 
node.</P>
<P>An example</P>
<TABLE align=center>
  <TBODY>
  <TR>
    <TD class=branchNode>...<BR>|<BR>1/2<BR>|</TD></TR>
  <TR>
    <TD id=12><A class=exNode>Income</A></TD></TR>
  <TR>
    <TD align=middle>
      <TABLE>
        <TBODY><BR>
        <TR>
          <TD style="BORDER-BOTTOM: #000000 1px solid" colSpan=2>|</TD></TR>
        <TR>
          <TD vAlign=top>
            <TABLE>
              <TBODY><BR>
              <TR>
                <TD class=branchNode>|<BR>High<BR>1/2<BR>|</TD></TR>
              <TR>
                <TD id=1220><A class=exNode>District</A></TD></TR>
              <TR>
                <TD align=middle>
                  <TABLE>
                    <TBODY><BR>
                    <TR>
                      <TD style="BORDER-BOTTOM: #000000 1px solid" 
                      colSpan=3>|</TD></TR>
                    <TR>
                      <TD vAlign=top>
                        <TABLE>
                          <TBODY><BR>
                          <TR>
                            <TD 
                          class=branchNode>|<BR>Suburban<BR>0/0<BR>|</TD></TR>
                          <TR>
                            <TD id=122000><SPAN 
                              class=leafNode>null</SPAN><BR><SPAN 
                              class=correctStat>0:0</SPAN><BR><SPAN 
                              class=wrongStat>0:1</SPAN></TD></TR></TBODY></TABLE></TD>
                      <TD vAlign=top>
                        <TABLE>
                          <TBODY><BR>
                          <TR>
                            <TD class=branchNode>|<BR>Rural<BR>1/1<BR>|</TD></TR>
                          <TR>
                            <TD id=122001><SPAN 
                              class=leafNode>Responded</SPAN><BR><SPAN 
                              class=correctStat>1:0</SPAN><BR><SPAN 
                              class=wrongStat>0:0</SPAN></TD></TR></TBODY></TABLE></TD>
                      <TD vAlign=top>
                        <TABLE>
                          <TBODY><BR>
                          <TR>
                            <TD class=branchNode>|<BR>Urban<BR>1/1<BR>|</TD></TR>
                          <TR>
                            <TD id=122002><SPAN 
                              class=leafNode>Nothing</SPAN><BR><SPAN 
                              class=correctStat>1:0</SPAN><BR><SPAN 
                              class=wrongStat>0:0</SPAN></TD></TR></TBODY></TABLE></TD></TR></TBODY></TABLE></TD></TR></TBODY></TABLE></TD>
          <TD vAlign=top>
            <TABLE>
              <TBODY><BR>
              <TR>
                <TD class=branchNode>|<BR>Low<BR>0/0<BR>|</TD></TR>
              <TR>
                <TD id=1221><SPAN class=leafNode>null</SPAN><BR><SPAN 
                  class=correctStat>0:0</SPAN><BR><SPAN 
                  class=wrongStat>0:2</SPAN></TD></TR></TBODY></TABLE></TD></TR></TBODY></TABLE></TD></TR></TBODY></TABLE>
<P>The above node labelled 'Income' can be seen to produce branches that lead to 
a total of 3 misclassifications on the testing data (the second red number on 
leaf nodes). Replacing this node just with a leaf node labelled 'Responded' 
would result in: </P>
<TABLE align=center>
  <TBODY><BR>
  <TR>
    <TD class=branchNode>...<BR>|<BR>1/2<BR>|</TD></TR>
  <TR>
    <TD><SPAN class=leafNode>Responded</SPAN><BR><SPAN 
      class=correctStat>1:2</SPAN><BR><SPAN 
  class=wrongStat>1:1</SPAN></TD></TR></TBODY></TABLE>
<P>Although now we have a re-substitution error on the training set, the errors 
on the testing set are reduced. This is desirable in terms of accuracy and tree 
size. </P>
<H3>Other stategies to avoid overfitting </H3>
<P>Pruning, or post-pruning as we have described here, is not the only method 
that has been used to avoid overfitting. Various other post-pruning methods 
exist, including strategies that convert the tree to rules before pruning. 
Recent work has involved trying to incorporate some overfitting-prevention bias 
into the splitting part of the algorithm. One example of this is based on the 
<STRONG>minimum-description length</STRONG> principle (Risanen 85) that states 
that the best hypothesis is the one that minimises length of encoding of the 
hypothesis and data. This has been shown to produce accurate trees with small 
size (e.g. see Quinlan 89 and Mehta et. al. 95).</P>
<H3>Try it out</H3>
<P>The next exercise allows you to prune branches of any trees that you can 
build. You can replace any non-leaf node with a leaf. See how this works in 
practice. See if you can you reduce the testing misclassification errors by 
pruning.</P>
<P><STRONG>References</STRONG></P>
<P><STRONG>Mingers, J., 1989. An empirical comparison of pruning methods for 
decisiontree induction.</STRONG> <BR>Machine Learning 4 (2), 227–243.</P>
<P><STRONG>Esposito, F., Malerba, D., Semeraro, G., 1997. A comparative analysis 
of methods<BR>for pruning decision trees.</STRONG> <BR>IEEE Transactions on 
Pattern Machine Intelligence 19 (5), 476–491.<BR>URL: <A class=normallink 
href="http://citeseer.nj.nec.com/esposito97comparative.html">http://citeseer.nj.nec.com/esposito97comparative.html</A></P>
<P><STRONG>Rissanen, J., 1985. The minimum description length 
principle</STRONG>. In: Kotz, S.,<BR>Johnson, N. (Eds.), Encyclopedia of 
Statistical Sciences. Vol. 5. John Wiley<BR>and sons, New York, pp. 523–527.</P>
<P><STRONG>Quinlan, J., Rivest, R., 1989. Inferring decision trees using minimum 
description length principle.</STRONG> <BR>Information and Computation.</P>
<P><STRONG>Mehta, M., Rissanen, J., Agrawal, R., 1995. MDL-based decision tree 
pruning.</STRONG><BR>In: Proceedings of the First International Conference on 
Knowledge Discovery<BR>and Data Mining (KDD’95). pp. 216–221.<BR>URL: <A 
class=normallink 
href="http://citeseer.nj.nec.com/mehta95mdlbased.html">http://citeseer.nj.nec.com/mehta95mdlbased.html</A> 
</P>
<DIV class=book>
<DIV class=nav>
<DIV class=links>
<DIV class=prev><A title="View the previous page." 
href="http://decisiontrees.net/node/37">previous</A></DIV>
<DIV class=next><A title="View the next page." 
href="http://decisiontrees.net/node/38">next</A></DIV>
<DIV class=up><A title="View this page's parent section." 
href="http://decisiontrees.net/node/16">up</A></DIV></DIV>
<DIV class=titles>
<DIV class=prev>Tutorial (13): Overfitting</DIV>
<DIV class=next>Tutorial (15): Exercise 6</DIV></DIV></DIV></DIV></DIV>
<DIV class=links><A 
title="Show a printer-friendly version of this book page and its sub-pages." 
href="http://decisiontrees.net/book/print/44">printer-friendly version</A> – <A 
title="Share your thoughts and opinions related to this posting." 
href="http://decisiontrees.net/comment/reply/44#comment">add new 
comment</A></DIV></DIV><A id=comment></A>
<FORM action=comment method=post>
<DIV><INPUT type=hidden value=44 name=edit[nid]> </DIV></FORM><!-- end content -->
<DIV class=footer-both id=footer>
<P>
<SCRIPT type=text/javascript><!--
google_ad_client = "pub-5329483376432160";
google_ad_width = 468;
google_ad_height = 60;
google_ad_format = "468x60_as";
google_ad_type = "text";
google_ad_channel ="";
google_color_border = "336699";
google_color_bg = "FFFFFF";
google_color_link = "CF094A";
google_color_url = "8BAEC9";
google_color_text = "000000";
//--></SCRIPT>

<SCRIPT src="Tutorial (14) Pruning  Decision Trees_files/show_ads.js" 
type=text/javascript>
</SCRIPT>
</P></DIV></DIV></DIV></DIV></DIV>
<DIV class=sidebar id=sidebar-right>
<DIV class="block block-block" id=block-block-1>
<H2 class=first>Adverts</H2>
<DIV class=content><BR>
<SCRIPT type=text/javascript><!--
google_ad_client = "pub-5329483376432160";
google_ad_width = 160;
google_ad_height = 600;
google_ad_format = "160x600_as";
google_ad_type = "text_image";
google_ad_channel ="8309837109";
google_color_border = "336699";
google_color_bg = "FFFFFF";
google_color_link = "CF094A";
google_color_url = "8BAEC9";
google_color_text = "000000";
//--></SCRIPT>
<BR>
<SCRIPT src="Tutorial (14) Pruning  Decision Trees_files/show_ads.js" 
type=text/javascript>
</SCRIPT>
<BR></DIV></DIV></DIV><SPAN 
class=clear></SPAN></DIV></DIV></DIV></DIV></DIV></DIV></DIV></DIV></DIV></DIV></DIV></DIV>
<DIV class=end-both id=end>
<DIV class=ew1>
<DIV class=ew2></DIV></DIV></DIV></BODY></HTML>
