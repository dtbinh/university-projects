<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN" "http://www.w3c.org/TR/1999/REC-html401-19991224/loose.dtd">
<!-- saved from url=(0032)http://decisiontrees.net/node/27 -->
<HTML lang=en xml:lang="en" xmlns="http://www.w3.org/1999/xhtml"><HEAD><TITLE>Tutorial (4): ID3 | Decision Trees</TITLE>
<META http-equiv=Content-Style-Type content=text/css>
<META http-equiv=Content-Type content="text/html; charset=utf-8">
<STYLE type=text/css media=all>@import url( misc/drupal.css );
</STYLE>

<STYLE type=text/css media=all>@import url( themes/friendselectric/style.css );
</STYLE>

<SCRIPT type=text/javascript> </SCRIPT>

<META content="MSHTML 6.00.2900.2873" name=GENERATOR></HEAD>
<BODY>
<DIV class=bw1>
<DIV class=bw2>
<DIV id=body-wrap>
<DIV id=header>
<DIV class=hw1>
<DIV class=hw2><A title="Index Page" href="http://decisiontrees.net/"><IMG 
id=site-logo alt=Logo src="Tutorial (4) ID3  Decision Trees_files/logo.png"></A> 

<H1 class=without-slogan id=site-name><A title="Index Page" 
href="http://decisiontrees.net/">Decision Trees</A></H1>
<DIV id=top-nav>
<UL id=primary>
  <LI><A title="Interactive tutorial" 
  href="http://decisiontrees.net/node/16"><SPAN class=lw1><SPAN 
  class=lw2>Tutorial</SPAN></SPAN></A> </LI>
  <LI><A title=Books href="http://decisiontrees.net/node/19"><SPAN 
  class=lw1><SPAN class=lw2>Books</SPAN></SPAN></A> </LI>
  <LI><A title=Software href="http://decisiontrees.net/node/22"><SPAN 
  class=lw1><SPAN class=lw2>Software</SPAN></SPAN></A> </LI>
  <LI><A title=Sites href="http://decisiontrees.net/node/23"><SPAN 
  class=lw1><SPAN class=lw2>Sites</SPAN></SPAN></A> </LI>
  <LI><A title=Papers href="http://decisiontrees.net/node/45"><SPAN 
  class=lw1><SPAN class=lw2>Papers</SPAN></SPAN></A> </LI>
  <LI><A title=Forums href="http://decisiontrees.net/forum"><SPAN 
  class=lw1><SPAN class=lw2>Forums</SPAN></SPAN></A> </LI>
  <LI><A title=About href="http://decisiontrees.net/node/42"><SPAN 
  class=lw1><SPAN class=lw2>About</SPAN></SPAN></A> 
</LI></UL></DIV></DIV></DIV></DIV>
<DIV class=content-both id=content>
<DIV class=cw1>
<DIV class=cw2>
<DIV class=cw3>
<DIV class=cw4>
<DIV class=cw5>
<DIV class=cw6>
<DIV class=cw7>
<DIV class=cw8>
<DIV class=content-wrap-both id=content-wrap>
<DIV class=sidebar id=sidebar-left>
<DIV class="block block-user" id=block-user-0>
<H2 class=first>User login</H2>
<DIV class=content>
<FORM action=user/login?destination=node%2F27 method=post>
<DIV class=user-login-block>
<DIV class=form-item><LABEL for=edit-name>Username:</LABEL><BR><INPUT 
class=form-text id=edit-name maxLength=64 size=15 name=edit[name]> </DIV>
<DIV class=form-item><LABEL for=edit-pass>Password:</LABEL><BR><INPUT 
class=form-password id=edit-pass type=password maxLength=64 size=15 
name=edit[pass]> </DIV><INPUT class=form-submit type=submit value="Log in" name=op> </DIV></FORM>
<DIV class=item-list>
<UL>
  <LI><A title="Create a new user account." 
  href="http://decisiontrees.net/user/register">Create new account</A>
  <LI><A title="Request new password via e-mail." 
  href="http://decisiontrees.net/user/password">Request new 
password</A></LI></UL></DIV></DIV></DIV>
<DIV class="block block-book" id=block-book-0>
<H2>Tutorial</H2>
<DIV class=content>
<DIV class=menu>
<UL>
  <LI class=leaf><A href="http://decisiontrees.net/node/21">Tutorial (1): A 
  simple decision tree</A>
  <LI class=leaf><A href="http://decisiontrees.net/node/25">Tutorial (2): 
  Exercise 1</A>
  <LI class=leaf><A href="http://decisiontrees.net/node/26">Tutorial (3): 
  Occam's Razor</A>
  <LI class=leaf><A class=active 
  href="http://decisiontrees.net/node/27">Tutorial (4): ID3</A>
  <LI class=leaf><A href="http://decisiontrees.net/node/28">Tutorial (5): 
  Exercise 2</A>
  <LI class=leaf><A href="http://decisiontrees.net/node/29">Tutorial (6): 
  Entropy Bias</A>
  <LI class=leaf><A href="http://decisiontrees.net/node/31">Tutorial (7): 
  Exercise 3</A>
  <LI class=leaf><A href="http://decisiontrees.net/node/32">Tutorial (8): Other 
  Splitting Criteria</A>
  <LI class=leaf><A href="http://decisiontrees.net/node/33">Tutorial (9): 
  Exercise 4</A>
  <LI class=leaf><A href="http://decisiontrees.net/node/34">Tutorial (10): 
  Advanced Topics</A>
  <LI class=leaf><A href="http://decisiontrees.net/node/35">Tutorial (11): 
  Evaluating Decision Trees</A>
  <LI class=leaf><A href="http://decisiontrees.net/node/36">Tutorial (12): 
  Exercise 5</A>
  <LI class=leaf><A href="http://decisiontrees.net/node/37">Tutorial (13): 
  Overfitting</A>
  <LI class=leaf><A href="http://decisiontrees.net/node/44">Tutorial (14): 
  Pruning</A>
  <LI class=leaf><A href="http://decisiontrees.net/node/38">Tutorial (15): 
  Exercise 6</A>
  <LI class=leaf><A href="http://decisiontrees.net/node/39">Tutorial (16): 
  Further Topics</A>
  <LI class=leaf><A href="http://decisiontrees.net/node/40">Tutorial (17): 
  Conclusion</A></LI></UL></DIV></DIV></DIV>
<DIV class="block block-user" id=block-user-1>
<H2>Navigation</H2>
<DIV class=content>
<DIV class=menu>
<UL>
  <LI class=collapsed><A href="http://decisiontrees.net/node/add"><SPAN 
  class=lw1>create content</SPAN></A> </LI></UL></DIV></DIV></DIV>
<DIV class="block block-forum" id=block-forum-0>
<H2>Active forum topics</H2>
<DIV class=content>
<DIV class=item-list>
<UL>
  <LI><A href="http://decisiontrees.net/node/47">Machine Learning Data 
  Sets</A></LI></UL></DIV>
<DIV class=more-link><A title="Read the latest forum topics." 
href="http://decisiontrees.net/forum">more</A></DIV></DIV></DIV>
<DIV class="block block-comment" id=block-comment-0>
<H2>Recent comments</H2>
<DIV class=content>
<DIV class=item-list>
<UL>
  <LI><A href="http://decisiontrees.net/node/20#comment-17">automatic 
  interaction detection</A><BR>8 weeks 3 days ago
  <LI><A href="http://decisiontrees.net/node/20#comment-16">Data sets</A><BR>8 
  weeks 6 days ago
  <LI><A href="http://decisiontrees.net/node/20#comment-15">CHAID AND 
  AID</A><BR>9 weeks 1 hour ago
  <LI><A href="http://decisiontrees.net/node/20#comment-14">re: CHAID and 
  AID</A><BR>9 weeks 1 hour ago
  <LI><A href="http://decisiontrees.net/node/20#comment-13">CHAID and 
  AID</A><BR>9 weeks 5 hours ago
  <LI><A href="http://decisiontrees.net/node/45#comment-12">Large database 
  decision tree classifiers</A><BR>9 weeks 3 days ago
  <LI><A href="http://decisiontrees.net/node/22#comment-9">Re:See5</A><BR>10 
  weeks 15 hours ago
  <LI><A href="http://decisiontrees.net/node/45#comment-8">Need papers about 
  data mining</A><BR>10 weeks 22 hours ago
  <LI><A href="http://decisiontrees.net/node/22#comment-7">See5</A><BR>10 weeks 
  22 hours ago
  <LI><A href="http://decisiontrees.net/node/20#comment-6">Access 
  problems?</A><BR>10 weeks 22 hours ago</LI></UL></DIV></DIV></DIV>
<DIV class="block block-user" id=block-user-3>
<H2>Who's online</H2>
<DIV class=content>There are currently 0 users and 14 guests 
online.</DIV></DIV></DIV>
<DIV class=main-both id=main>
<DIV class=main-wrap-both id=main-wrap>
<DIV class=mw1>
<DIV class=breadcrumb><A href="http://decisiontrees.net/">Home</A> » <A 
href="http://decisiontrees.net/node/16"><SPAN 
class=lw1>Tutorial</SPAN></A></DIV>
<H2 class=main-title>Tutorial (4): ID3</H2><!-- begin content -->
<DIV class=node>
<DIV class=content>
<H1>Decision Trees tutorial &gt; ID3 &amp; Entropy</H1>
<H2>ID3</H2>
<P>An early technique by the influential Ross Quinlan that influenced a large 
part of the research on Decision Trees is useful to look at in order to 
understand basic decision tree construction. </P>
<H3>Splitting Criteria</H3>
<P>A fundamental part of any algorithm that constructs a decision tree from a 
dataset is the method in which it selects attributes at each node of the tree. 
In the <A class=normallink href="http://decisiontrees.net/node/26">previous 
exercise</A>, you may have found it was better to place certain attributes (such 
as 'District') higher up the tree in order to produce a short tree. <SPAN 
class=titles>Q. Why is this? </SPAN></P>
<P>A. Some attributes split the data up more purely than others. That means that 
their values correspond more consistently with instances that have particular 
values of the target attribute (the one we want to predict) than those of 
another attribute. Therefore, we might say that such attributes have some 
underlying relationship with the target attribute . But how can this be 
quantified in some way? Essentially, we would like some sort of measure that 
enables us to compare attributes with each other and then be able to decide to 
put ones that split the data more purely higher up the tree.</P>
<H3>Entropy</H3>
<P>A measure used from Information Theory in the ID3 algorithm and many others 
used in decision tree construction is that of Entropy. Informally, the entropy 
of a dataset can be considered to be how disordered it is. It has been shown 
that entropy is related to information, in the sense that the higher the 
entropy, or uncertainty, of some data, then the more information is required in 
order to completely describe that data. In building a decision tree, we aim to 
decrease the entropy of the dataset until we reach leaf nodes at which point the 
subset that we are left with is pure, or has zero entropy and represents 
instances all of one class (all instances have the same value for the target 
attribute). </P>
<P>We measure the entropy of a dataset,S, with respect to one attribute, in this 
case the target attribute, with the following calculation:</P>
<P class=titles align=center><IMG class=eq height=59 alt="Entropy Calculation" 
src="Tutorial (4) ID3  Decision Trees_files/entropyEq.gif" width=240><BR>where 
Pi is the proportion of instances in the dataset that take the ith value of the 
target attribute </P>
<P>This probability measures give us an indication of how uncertain we are about 
the data. And we use a log2 measure as this represents how many bits we would 
need to use in order to specify what the class (value of the target attribute) 
is of a random instance. </P>
<P>Using the example of the marketing data, we know that there are two classes 
in the data and so we use the fractions that each class represents in an entropy 
calculation:</P>
<P align=center><SPAN class=titles>Entropy (S = [9/14 responses, 5/14 no 
responses])<BR><BR></SPAN>= -9/14 log2 9/14 - 5/14 log2 5/14 = <SPAN 
class=titles>0.947 bits </SPAN></P>
<P>Ok now we want a quantitative way of seeing the effect of splitting the 
dataset by using a particular attribute (which is part of the tree building 
process). We can use a measure called <SPAN class=titles>Information 
Gain</SPAN>, which calculates the <SPAN class=titles>reduction in entropy 
(<EM>Gain in information</EM>)</SPAN> that would result on splitting the data on 
an attribute, A.</P>
<P align=center><IMG class=eq height=64 alt="Gain Calulation" 
src="Tutorial (4) ID3  Decision Trees_files/gainEq_0.gif" width=475><BR><SPAN 
class=titles>where v is a value of A<BR>, |Sv| is the subset of instances of S 
where A takes the value v, <BR>and |S| is the number of instances </SPAN></P>
<P>Continuing with our example dataset, let's name it S just for convenience, 
let's work out the Information Gain that splitting on the attribute District 
would result in over the entire dataset: </P>
<P align=center><IMG class=eq height=106 alt="Gain Example" 
src="Tutorial (4) ID3  Decision Trees_files/gainCalc.gif" width=563></P>
<P>So by calculating this value for each attribute that remains, we can see 
which attribute splits the data more purely. Let's imagine we want to select an 
attribute for the root node, then performing the above calcualtion for all 
attributes gives:</P>
<UL>
  <LI class=titles>Gain(S,House Type) = 0.049 bits 
  <LI class=titles>Gain(S,Income) =0.151 bits 
  <LI class=titles>Gain(S,Previous Customer) = 0.048 bits </LI></UL>
<P>We can clearly see that District results in the highest reduction in entropy 
or the highest information gain. We would therefore choose this at the root node 
splitting the data up into subsets corresponding to all the different values for 
the District attribute. </P>
<P>With this node evaluation technique we can procede recursively through the 
subsets we create until leaf nodes have been reached throughout and all subsets 
are pure with zero entropy. This is exactly how ID3 and other variants work.</P>
<H3>Decision Tree Construction Algorithm</H3>
<P>We can now present the basic ID3 algorithm in pseudo-code:</P>
<DIV class=headers>
<P><STRONG>Input: </STRONG>A data set, S <BR><STRONG>Output: </STRONG>A decision 
tree </P>
<UL>
  <LI>If all the instances have the same value for the target attribute then 
  return a decision tree that is simply this value (not really a tree - more of 
  a stump). 
  <LI>Else 
  <OL>
    <LI>Compute Gain values (see above) for all attributes and select an 
    attribute with the lowest value and create a node for that attribute. 
    <LI>Make a branch from this node for every value of the attribute 
    <LI>Assign all possibe values of the attribute to branches. 
    <LI>Follow each branch by partitioning the dataset to be only instances 
    whereby the value of the branch is present and then go back to 1. 
  </LI></OL></LI></UL></DIV>
<P>Ok, so go and play about with this method on the next page interactively. You 
can see how easy it now is to construct a small decision tree with this entropy 
measure as a guide. </P>
<DIV class=book>
<DIV class=nav>
<DIV class=links>
<DIV class=prev><A title="View the previous page." 
href="http://decisiontrees.net/node/26">previous</A></DIV>
<DIV class=next><A title="View the next page." 
href="http://decisiontrees.net/node/28">next</A></DIV>
<DIV class=up><A title="View this page's parent section." 
href="http://decisiontrees.net/node/16">up</A></DIV></DIV>
<DIV class=titles>
<DIV class=prev>Tutorial (3): Occam's Razor</DIV>
<DIV class=next>Tutorial (5): Exercise 2</DIV></DIV></DIV></DIV></DIV>
<DIV class=links><A 
title="Show a printer-friendly version of this book page and its sub-pages." 
href="http://decisiontrees.net/book/print/27">printer-friendly version</A> – <A 
title="Share your thoughts and opinions related to this posting." 
href="http://decisiontrees.net/comment/reply/27#comment">add new 
comment</A></DIV></DIV><A id=comment></A>
<FORM action=comment method=post>
<DIV><INPUT type=hidden value=27 name=edit[nid]> </DIV></FORM><!-- end content -->
<DIV class=footer-both id=footer>
<P>
<SCRIPT type=text/javascript><!--
google_ad_client = "pub-5329483376432160";
google_ad_width = 468;
google_ad_height = 60;
google_ad_format = "468x60_as";
google_ad_type = "text";
google_ad_channel ="";
google_color_border = "336699";
google_color_bg = "FFFFFF";
google_color_link = "CF094A";
google_color_url = "8BAEC9";
google_color_text = "000000";
//--></SCRIPT>

<SCRIPT src="Tutorial (4) ID3  Decision Trees_files/show_ads.js" 
type=text/javascript>
</SCRIPT>
</P></DIV></DIV></DIV></DIV></DIV>
<DIV class=sidebar id=sidebar-right>
<DIV class="block block-block" id=block-block-1>
<H2 class=first>Adverts</H2>
<DIV class=content><BR>
<SCRIPT type=text/javascript><!--
google_ad_client = "pub-5329483376432160";
google_ad_width = 160;
google_ad_height = 600;
google_ad_format = "160x600_as";
google_ad_type = "text_image";
google_ad_channel ="8309837109";
google_color_border = "336699";
google_color_bg = "FFFFFF";
google_color_link = "CF094A";
google_color_url = "8BAEC9";
google_color_text = "000000";
//--></SCRIPT>
<BR>
<SCRIPT src="Tutorial (4) ID3  Decision Trees_files/show_ads.js" 
type=text/javascript>
</SCRIPT>
<BR></DIV></DIV></DIV><SPAN 
class=clear></SPAN></DIV></DIV></DIV></DIV></DIV></DIV></DIV></DIV></DIV></DIV></DIV></DIV>
<DIV class=end-both id=end>
<DIV class=ew1>
<DIV class=ew2></DIV></DIV></DIV></BODY></HTML>
