<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN">
<!-- saved from url=(0047)http://intsys.mgt.qub.ac.uk/notes/backprop.html -->
<HTML><HEAD><TITLE>Perceptron and back-prop</TITLE>
<META http-equiv=Content-Type content="text/html; charset=windows-1252"><LINK 
href="Perceptron and back-prop_files/nn.css" rel=STYLESHEET>
<META content="MSHTML 6.00.2900.2873" name=GENERATOR></HEAD>
<BODY>
<H2><A href="http://intsys.mgt.qub.ac.uk/notes/nn.html"><IMG height=16 alt="<-" 
src="Perceptron and back-prop_files/left.gif" width=32 align=bottom 
border=0></A> TECHNIQUES: THE MULTI-LAYER PERCEPTRON<A name=top></A></H2>
<UL>
  <LI><A 
  href="http://intsys.mgt.qub.ac.uk/notes/backprop.html#learning">Learning</A> 
  <LI><A href="http://intsys.mgt.qub.ac.uk/notes/backprop.html#backprop">Back 
  propagation</A> </LI></UL>
<P align=center><IMG height=270 src="Perceptron and back-prop_files/nn04.gif" 
width=412><BR>The General Multi-Layer Perceptron</P>
<UL>
  <LI>one of the most important and widely used network models 
  <LI>links together processing units into a network made up of layers 
  <UL>
    <LI>input (set by problem data) 
    <LI>output (of solution values) 
    <LI>typically one or two hidden layers (units model patterns in input data) 
    </LI></UL>
  <LI>Each layer is fully connected to the succeeding layer </LI></UL>
<P>The network is <B><I>feedforward</I></B>. When using or testing a trained 
network, the input values set the values of elements in the first hidden layer, 
which influence the next layer, and so on until it sets values for the output 
layer elements. See a <A href="http://intsys.mgt.qub.ac.uk/notes/feedforw.html" 
name=feedforward>JavaScript model of the feedforward calculation</A> in the 
Neural Planner diggers example. </P>
<P>What if a known input pattern produces a wildly incorrect output signal? Then 
we need to train the network, through a learning process. </P>
<HR>

<H3><A href="http://intsys.mgt.qub.ac.uk/notes/backprop.html#top" 
name=learning><IMG height=16 alt="<-" 
src="Perceptron and back-prop_files/left.gif" width=32 align=bottom 
border=0></A> Learning</H3>
<P>in neural networks is done by changing the weighting factors (weights) at 
each element to reduce output errors. I've put together a simple JavaScript 
demonstration of <A 
href="http://intsys.mgt.qub.ac.uk/notes/perceptr.html">learning in a 
single-layer perceptron</A>. </P>
<P><IMG height=452 src="Perceptron and back-prop_files/biascorr.gif" 
width=452></P>
<P>In MLPs, learning is <B><I>supervised</I></B>, with separate training and 
recall phases. For an example of how you train and then test a network, see Bob 
Mitchell's <A 
href="http://www-cse.uta.edu/~cook/ai1/lectures/applets/hnn/JRec.html">handwriting 
recognizer</A> </P>
<P>During training the nodes in the hidden layers organise themselves such that 
different nodes learn to recognise different features of the total input space. 
</P>
<P>During the recall phase of operation the network will respond to inputs that 
exhibit features similar to those learned during training. Incomplete or noisy 
inputs may be completely recovered by the network.</P>
<P>In its learning phase, you give it a training set of examples with known 
inputs and outputs.</P>
<P><IMG height=221 src="Perceptron and back-prop_files/bpnet.gif" width=439 
align=bottom></P><IMG height=195 alt="error vs. weights graph" 
src="Perceptron and back-prop_files/desw.gif" width=271 align=right border=0> 
<OL>
  <LI>For each input pattern, the network produces an output pattern. 
  <LI>It compares the actual output and the desired one from the training set 
  and calculates an error. 
  <LI>It adjusts its weights a little to reduce the error (sliding down the 
  slope). 
  <LI>It repeats 1-3 many times for every example in the training set until it 
  has minimised the errors. </LI></OL>
<P>For a graphical visualisation of how a neural network gradually adjusts 
itself to get closer and closer to the input patterns, see Jochen Fröhlich's 
Java simulation of a <A 
href="http://rfhs8012.fh-regensburg.de/~saj39122/jfroehl/diplom/e-sample.html">self-organising 
Kohonen feature map</A>. (Note that this uses a different learning algorithm, 
not back propagation, but it still gradually gets closer to the training set 
values.)</P>
<P>There are many weights to be adjusted, so consider a multi-dimensional 
surface constructed by plotting the total network error in weight space (i.e. 
over all the possible changes in weight). A 3-D approximation could look like 
this:</P><IMG height=198 src="Perceptron and back-prop_files/errsurf.gif" 
width=316 align=left> <BR clear=right>
<P>During training:</P>
<UL>
  <LI>objective: find the global minimum on the error surface. 
  <LI>solution: <STRONG>gradient descent</STRONG>, adjust weights to follow the 
  steepest downhill slope. 
  <LI>don't know surface in advance, so explore it in many small steps. 
</LI></UL><BR clear=all>
<HR>

<H3><A href="http://intsys.mgt.qub.ac.uk/notes/backprop.html#top" 
name=backprop><IMG height=16 alt="<-" 
src="Perceptron and back-prop_files/left.gif" width=32 align=bottom 
border=0></A> Back propagation</H3>
<UL>
  <LI>During training, information is propagated back through the network and 
  used to update connection weights. How? 
  <LI>Different neural network architectures use different algorithms to 
  calculate the weight changes. 
  <LI>Backpropagation (BP) is a commonly used (but inefficient) algorithm in 
  MLPs. 
  <LI>We know the errors at the output layer, but not at the hidden layer 
  elements. 
  <LI>BP solves the problem of how to calculate the hidden layer errors (it 
  propagates the output errors back to the previous layer using the output 
  element weights). </LI></UL>
<P>The mathematics of this algorithm are given in several textbooks and on-line 
tutorials. For a detailed explanation of the back propagation algorithm, see 
Carling, Alison (1992) <CITE>Introducing Neural Networks</CITE>, Wilmslow: Sigma 
Press, pp. 147-154.</P>
<P>It helps to know some features of it when training neural networks.</P>
<OL>
  <LI>Internally most BP networks work with values between 0 and 1. If your 
  inputs have a different range, NN simulators like Neural Planner will scale 
  each input variable minimum to 0 and maximum to 1.
  <P></P>
  <LI>They change the weights each time by some fraction of the change needed to 
  completely correct the error. This fraction, ß, is the <STRONG>learning 
  rate</STRONG>.<BR><IMG height=210 
  src="Perceptron and back-prop_files/nnslosh.gif" width=438 align=bottom> 
  <OL type=a>
    <LI>High learning rates cause the learning algorithm to take large steps on 
    the error surface, with the risk of missing a minimum, or unstably 
    oscillating across the error minimum ('sloshing') 
    <LI>Small steps, from a low learning rate, eventually find a minimum, but 
    they take a long time to get there. 
    <LI>Some NN simulators can be set to reduce the learning rate as the error 
    decreases. 
    <LI>Also, sloshing can be reduced by mixing in to the weight change a 
    proportion of the last weight change, so smoothing out small fluctutions. 
    This proportion is the <STRONG>momentum</STRONG> term. </LI></OL>
  <P><IMG height=213 src="Perceptron and back-prop_files/localmin.gif" width=258 
  align=right> </P>
  <LI>The algorithm finds the nearest local minimum, not always the lowest 
  minimum. 
  <P>One solution commonly used in backpropagation is to:</P>
  <OL>
    <LI>restart learning every so often from a new set of random weights (i.e. 
    somewhere else in the weight space). 
    <LI>find the local minimum from each new start 
    <LI>keep track of the best minimum found </LI></OL><BR clear=right>
  <LI><STRONG>Overfitting</STRONG> is when the NN learns the specific details of 
  the training set, instead of the general pattern found in all present and 
  future data 
  <P><IMG height=211 src="Perceptron and back-prop_files/xyfit.gif" width=264 
  align=bottom> <IMG height=211 src="Perceptron and back-prop_files/xyofit.gif" 
  width=264 align=bottom></P><IMG height=252 
  alt="training and test data set &#10;management" 
  src="Perceptron and back-prop_files/traintst.gif" width=333 align=right> There 
  can be two causes: 
  <OL type=a>
    <LI>Training for too long. Solution? 
    <OL>
      <LI>Test against a separate test set every so often. 
      <LI>Stop when the results on the test set start getting worse. </LI></OL>
    <LI>Too many hidden nodes 
    <UL>
      <LI>One node can model a linear function 
      <LI>More nodes can model higher-order functions, or more input patterns 
      <LI>Too many nodes model the training set too closely, preventing 
      generalisation. </LI></UL></LI></OL></LI></OL><BR clear=all>
<HR>

<H4>Learning parameters for Neural Networks</H4>
<P>A summary of the parameters used in BP networks to control the learning 
behaviour.</P>
<TABLE cellPadding=2 border=1>
  <CAPTION align=bottom>Adapted from: Joseph P. Biggus (1996) <CITE>Data mining 
  with Neural Networks.</CITE> New York: McGraw-Hill, p. 82.</CAPTION>
  <TBODY>
  <TR vAlign=top>
    <TH>Parameter </TH>
    <TH>Models </TH>
    <TH>Function </TH></TR>
  <TR vAlign=top>
    <TD>Learning rate </TD>
    <TD>All </TD>
    <TD>Controls the step size for weight adjustments. Decreases over time for 
      some types of NN. </TD></TR>
  <TR vAlign=top>
    <TD>Momentum </TD>
    <TD>Back propagation </TD>
    <TD>Smooths the effect of weight adjustments over time. </TD></TR>
  <TR vAlign=top>
    <TD>Error tolerance </TD>
    <TD>Back propagation </TD>
    <TD>Specifies how close the output value must be to the desired value 
      before the error is considered </TD></TR>
  <TR vAlign=top>
    <TD>Activation function </TD>
    <TD>All </TD>
    <TD>The function used at each neural processing unit to generate the 
      output signal from the weighted average of inputs. Most common is the 
      sigmoid function. </TD></TR></TBODY></TABLE>
<HR>

<UL class=footer>
  <LI><A href="http://intsys.mgt.qub.ac.uk/notes/nn.html">Neural nets</A> 
  <UL>
    <LI>Back to <A 
    href="http://intsys.mgt.qub.ac.uk/notes/nnbiol.html">Biological 
    foundations</A> 
    <LI>On to <A href="http://intsys.mgt.qub.ac.uk/notes/nn.html#applic">NN 
    applications</A> </LI></UL></LI></UL>
<ADDRESS>Prepared by Dr. David R. Newman. </ADDRESS></BODY></HTML>
