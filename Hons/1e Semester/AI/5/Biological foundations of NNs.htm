<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN">
<!-- saved from url=(0045)http://intsys.mgt.qub.ac.uk/notes/nnbiol.html -->
<HTML><HEAD><TITLE>Biological foundations of NNs</TITLE>
<META http-equiv=Content-Type content="text/html; charset=windows-1252"><LINK 
href="Biological foundations of NNs_files/intsys.css" rel=STYLESHEET>
<META content="MSHTML 6.00.2900.2873" name=GENERATOR></HEAD>
<BODY>
<H2><A href="http://intsys.mgt.qub.ac.uk/notes/nn.html"><IMG height=16 alt="<-" 
src="Biological foundations of NNs_files/left.gif" width=32 align=bottom 
border=0></A>Biological Foundations of Neural Nets and simple artificial 
models<A name=biol></A></H2>
<H3>THE BIOLOGICAL <A 
href="http://intsys.mgt.qub.ac.uk/notes/neuron.html">NEURON</A></H3>
<P>Most artificial neural networks are based to some degree on biological 
systems. The function of the Processing Elements is based loosely on the nerve 
cell.</P>
<P align=center><IMG height=283 
src="Biological foundations of NNs_files/bionn2.gif" width=432 align=bottom></P>
<UL>
  <LI>Three major components: 
  <UL>
    <LI>Cell Body 
    <LI>Dendrites 
    <LI>Axon </LI></UL>
  <LI>Connections between neurons are formed at <B><CITE>synapses</CITE></B> 
  <LI>Information is represented and transmitted by chemically generated 
  electrical activity within the cell. 
  <LI>Inputs to the neuron, through synaptic connections with other neurons are 
  largely of two types: 
  <UL>
    <LI>Excitatory 
    <UL>
      <LI>causing a reduction in the potential across the cell membrane </LI></UL>
    <LI>Inhibitory 
    <UL>
      <LI>causing an increase in the polarisation of the receiving nerve cell. 
      </LI></UL></LI></UL>
  <LI><B><CITE>Input potentials</CITE></B> are summed within the cell body. 
</LI></UL>
<P>If the total input potential is sufficient then the neuron <B><CITE>fires. 
</CITE></B>An <B><CITE>action potential</CITE></B> is generated and propagates 
down the axon towards the synaptic junctions with other nerve cells.</P>
<P>From an Engineering perspective:</P>
<DL>
  <DT>Dendrites 
  <DD>input receptors 
  <DT>Cell Body 
  <DD>accumulator (with threshold function) 
  <DT>Axon 
  <DD>output channel. </DD></DL>
<P><B><I>Neural information is frequency encoded. </I></B></P>
<P>
<HR>

<P></P>
<H3>NEURAL MODELLING</H3>
<P>McCullock and Pitts suggested the first synthetic neuron in the early 1940s. 
In the McCullock-Pitts model the artificial neuron produces a binary output 
whose value depends on the weighted sum of its inputs.</P>
<P>The information processing performed in this way may be crudely summarised as 
follows: signals (action-potentials) appear at the unit's inputs (synapses). The 
effect (PSP) each signal has may be approximated by multiplying the signal by 
some number or <EM>weight</EM> to indicate the strength of the synapse. The 
weighted signals are now summed to produce an overall unit <EM>activation</EM>. 
If this activation exceeds a certain threshold the unit produces a an output 
response. This functionality is captured in the artificial neuron known as the 
Threshold Logic Unit (TLU) originally proposed by McCulloch and Pitts.</P>
<P><IMG height=273 src="Biological foundations of NNs_files/TLU.gif" width=429 
align=bottom></P>
<P>We suppose there are <EM>n</EM> inputs with signals <STRONG>x<SUB>1</SUB>, 
x<SUB>2</SUB>, ... x<SUB>n</SUB></STRONG> and weights <STRONG>w<SUB>1</SUB>, 
w<SUB>2</SUB>, ... w<SUB>n</SUB></STRONG>. The signals take on the values '1' or 
'0' only. That is the signals are <EM>Boolean</EM> valued. (This allows their 
relation to digital logic circuits to be discussed). The activation <EM>a</EM>, 
is given by</P>
<BLOCKQUOTE>
  <P><STRONG>a = w<SUB>1</SUB>x<SUB>1</SUB> + w<SUB>2</SUB>x<SUB>2</SUB> + ... 
  w<SUB>n</SUB>x<SUB>n</SUB></STRONG> </P></BLOCKQUOTE>
<P>This may be represented more compactly as</P>
<BLOCKQUOTE><IMG height=39 
  src="Biological foundations of NNs_files/eqsigwx.gif" width=82 
align=bottom></BLOCKQUOTE>
<P>the output <EM>y</EM> is then given by thresholding the activation </P>
<BLOCKQUOTE>
  <TABLE cellSpacing=0 cellPadding=2 border=0>
    <TBODY>
    <TR>
      <TH align=right rowSpan=2>
        <P align=right>y = <FONT size=+3>{</FONT> </P></TH>
      <TH align=left>
        <P align=left>1 if a &gt;= ß </P></TH></TR>
    <TR>
      <TH align=left>
        <P align=left>0 if a &lt; ß </P></TH></TR></TBODY></TABLE></BLOCKQUOTE>
<P>The threshold <STRONG>ß</STRONG> will often be zero. The threshold function 
is sometimes called a <EM>step-function</EM> or <EM>hard-limiter</EM> for 
obvious reasons. If we are to push the analogy with real neurons, the presence 
of an action-potential is denoted by binary `1' and its absence by binary 
`0'.</P>
<P><IMG height=159 src="Biological foundations of NNs_files/activefn.gif" 
width=466 align=bottom></P>
<P>In artificial neural networks, different activation functions are used. NNs 
with the identity function only support linear models. The sigmoid function lets 
you model higher order functions. For an example of how this works, see a <A 
href="http://intsys.mgt.qub.ac.uk/notes/feedforw.html" name=feedforw>JavaScript 
model of a neuron in the diggers network example</A> from Neural Planner. </P>
<H3>Hebbian Learning</H3>
<P><IMG height=165 alt=salivation 
src="Biological foundations of NNs_files/nn03.gif" width=296 align=bottom></P>
<BLOCKQUOTE>
  <P>When an axon of cell A is near enough to excite a cell B and repeatedly or 
  persistently takes part in firing it, some growth process or metabolic change 
  takes place in one or both cells such that A's efficiency, as one of the cells 
  firing B, is increased (Hebb 1949)</P></BLOCKQUOTE>
<P>In an artificial neural network Hebbian learning is associated with varying 
the weighting factors, W<SUB>i</SUB>, between neurons. </P>
<P>The process of adjusting the weights is referred to as a <B><CITE>Learning 
Algorithm</CITE></B>.</P>
<P>There are two main phases in the operation of a network:</P>
<OL>
  <LI>Learning 
  <LI>Recall </LI></OL>
<P>During learning the connection weights are changed in response to stimuli 
presented at the input buffer and, optionally, the output buffer.</P>
<P><STRONG><A name=learn>Learning</A></STRONG> may be:</P>
<UL>
  <LI><B>Supervised</B><BR><SMALL>The trainer tells the network when to learn, 
  when to recall</SMALL> 
  <LI><B>Unsupervised</B><BR><SMALL>The network continually learns from 
  similarities in new patterns</SMALL> 
  <LI>or <B>Reinforcement</B><BR><SMALL>During use the trainer tells the network 
  which are the best and worst cases to learn from<BR>(e.g. in <A 
  href="http://lslwww.epfl.ch/~aperez/BlackJack/classes/RLJavaBJ.html">learning 
  to play blackjack</A>)</SMALL> </LI></UL>
<P>You can step through an example of <A 
href="http://intsys.mgt.qub.ac.uk/notes/perceptr.html">supervised learning in a 
perceptron</A> (an early, simple, NN model), run Fred Corbett's <A 
href="http://diwww.epfl.ch/mantra/tutorial/english/perceptron/html/">Perceptron 
Learning Applet</A> (needs Java), see a graphical visualisation of a <A 
href="http://rfhs8012.fh-regensburg.de/~saj39122/jfroehl/diplom/e-sample.html">self-organising 
Kohonen feature map</A> from Jochen Fröhlich's <A 
href="http://rfhs8012.fh-regensburg.de/~saj39122/jfroehl/diplom/e-index.html">thesis</A>, 
or train and test a <A 
href="http://www-cse.uta.edu/~cook/ai1/lectures/applets/hnn/JRec.html">handwriting 
recogniser</A>.</P>
<P><STRONG>Recall</STRONG> refers to how the network processes a stimulus and 
produces a response. </P>
<P>The input may have been in the original training set or may have been 
previously unseen.</P>
<UL class=footer>
  <LI><A href="http://intsys.mgt.qub.ac.uk/notes/index.html">Intelligent 
  Systems</A> 
  <UL>
    <LI><A href="http://intsys.mgt.qub.ac.uk/notes/advankbs.html#4.2">Advanced 
    Intelligent Systems</A> 
    <UL>
      <LI><A href="http://intsys.mgt.qub.ac.uk/notes/nn.html">Neural nets</A> 
      </LI></UL></LI></UL></LI></UL>
<P class=footer>Prepared by Dr. David R. Newman.</P></BODY></HTML>
